# -*- coding: utf-8 -*-
"""churn-prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xFqemBpvnX4eF5xM44yeARv05LqnodnK
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

#connect google drive to colab notebook
from google.colab import drive
drive.mount("/content/gdrive")

3df=pd.read_csv("/content/gdrive/MyDrive/Colab Notebooks/ML Churn Prediction/data/IBM-Telco-data.csv")
df.head()

#Prints information of the dataset
df.info()

#Detects the null values
df.isnull()

#Changes the datatype from string to numeric(errors=coerce-replaces errors with null values as NaN)
df["TotalCharges"]=pd.to_numeric(df["TotalCharges"],errors="coerce")

#Detects the null values
df.isnull().sum()

#Delete the null values in a column in the dataset
df.dropna(subset=["TotalCharges"],inplace=True)

#Prints the first 5 rows
df.head()

#Deletes a column in a dataset(axis=1->column wise/axis=0->row wise)
df.drop('customerID',axis=1,inplace=True)

#Printing counts with respect to each column
for column in df.columns[1:]:
    print(column)
    print(df[column].value_counts())
    print("\n")

#library to visualize data
import seaborn as sns

#Creates Histogram for the dataset(bins refers to the number of bars )
df.hist(bins=50,figsize=(20,15))
plt.show()

object_columns = df.select_dtypes(include=['object']).columns

# Colorblind is used for people with different color vision in order to distinguish easily
sns.set_palette("colorblind")
for column in object_columns:
    #1 row 2 columns
    figure,ax=plt.subplots(1,2,figsize=(12,5))
    #ax[0]draw the countplot in the 0th cloumn
    sns.countplot(x=column,data=df,ax=ax[0])
    ax[0].set_title("Count of "+column)
    #hue assigns the color based on the column specified to it
    sns.countplot(x=column,data=df,hue='Churn',ax=ax[1])
    plt.show()

#mapping binary variables like male and female to 0's and 1's

df["gender"]=df["gender"].map({"Male":1, "Female":0})
df["Partner"]=df["Partner"].map({"Yes":1, "No":0})
df["Dependents"]=df["Dependents"].map({"Yes":1, "No":0})
df["PhoneService"]=df["PhoneService"].map({"Yes":1, "No":0})
df["PaperlessBilling"]=df["PaperlessBilling"].map({"Yes":1, "No":0})
df["Churn"]=df["Churn"].map({"Yes":1, "No":0})

df.head()

#The Categorical variables (yes or no values) is converted into numeric indicator variables
df=pd.get_dummies(df)
df=df.astype(float)

df.head()

#to divide code into training and testing
from sklearn.preprocessing import StandardScaler

X=df.drop("Churn",axis=1)
y=df["Churn"]

scalar=StandardScaler()
#Standardizing the features
X_scaled=scalar.fit_transform(X)

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(X_scaled,y,test_size=0.2,random_state=6)

#Accuracy tells you how many times the ML model was correct overall.
#Precision is how good the model is at predicting a specific category.
#Recall tells you how many times the model was able to detect a specific category.

#Logistic Regression
from sklearn.linear_model import LogisticRegression
log_reg=LogisticRegression()
log_reg.fit(X_train,y_train)
log_reg_score=log_reg.score(X_test,y_test)
print("Logistic Regression Accuracy: ",log_reg_score)

# Decision Tree
from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier()
dt.fit(X_train, y_train)
dt_score = dt.score(X_test, y_test)
print('Decision Tree accuracy: ', dt_score)

# K-nearest neighbors
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier()
knn.fit(X_train, y_train)
knn_score = knn.score(X_test, y_test)
print('K-nearest neighbors accuracy: ', knn_score)

#calculating all the metrics of the trained models
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Logistic Regression
log_reg_pred = log_reg.predict(X_test)
log_reg_acc = accuracy_score(y_test, log_reg_pred)
log_reg_prec = precision_score(y_test, log_reg_pred)
log_reg_recall = recall_score(y_test, log_reg_pred)
log_reg_f1 = f1_score(y_test, log_reg_pred)
print("Logistic Regression Metrics:")
print("Accuracy: ", log_reg_acc)
print("Precision: ", log_reg_prec)
print("Recall: ", log_reg_recall)
print("F1-Score: ", log_reg_f1)

# Decision Tree
dt_pred = dt.predict(X_test)
dt_acc = accuracy_score(y_test, dt_pred)
dt_prec = precision_score(y_test, dt_pred)
dt_recall = recall_score(y_test, dt_pred)
dt_f1 = f1_score(y_test, dt_pred)
print("Decision Tree Metrics:")
print("Accuracy: ", dt_acc)
print("Precision: ", dt_prec)
print("Recall: ", dt_recall)
print("F1-Score: ", dt_f1)

# K-nearest neighbors
knn_pred = knn.predict(X_test)
knn_acc = accuracy_score(y_test, knn_pred)
knn_prec = precision_score(y_test, knn_pred)
knn_recall = recall_score(y_test, knn_pred)
knn_f1 = f1_score(y_test, knn_pred)
print("K-nearest neighbors Metrics:")
print("Accuracy: ", knn_acc)
print("Precision: ", knn_prec)
print("Recall: ", knn_recall)
print("F1-Score: ", knn_f1)

#The above data is displayed in a table
metrics={"Model":["Logistic Regression","Decision Tree","K Nearest Neighbors"],
         "Accuracy":[log_reg_acc,dt_acc,knn_acc],
         "Precision":[log_reg_prec,dt_prec,knn_prec],
         "Recall":[log_reg_recall,dt_recall,knn_recall],
         "F1-Score":[log_reg_f1,dt_f1,knn_f1]}

metrics_df=pd.DataFrame(metrics)
metrics_df

#finding optimal hyper parameters through Cross-Validation
#Logistic Regression
from sklearn.model_selection import GridSearchCV

# Create a dictionary of the hyperparameters to tune
log_grid = {'C': [0.1, 1, 10], 'penalty': ['l1', 'l2']}

# Create a GridSearchCV object
log_reg_grid = GridSearchCV(log_reg, log_grid, cv=5)

# Fit the GridSearchCV object to the data
log_reg_grid.fit(X_train, y_train)

# Print the best hyperparameters
print(log_reg_grid.best_params_)

#Decision Tree
dt_grid = {'max_depth': [3, 5, 7, 10],
              'min_samples_leaf': [1, 2, 4]}

dt = DecisionTreeClassifier()

dt_search = GridSearchCV(dt, dt_grid, cv=5)
dt_search.fit(X_train, y_train)

print("Best parameters: {}".format(dt_search.best_params_))

#Knn
# Define the knn grid
knn_grid = {'n_neighbors': [3, 5, 7, 9], 'weights': ['uniform', 'distance']}

# Instantiate the KNN classifier
knn = KNeighborsClassifier()

# Create the grid search object
knn_search = GridSearchCV(knn, knn_grid, cv=5, scoring='accuracy')

# Fit the grid search to the data
knn_search.fit(X_train, y_train)

# Print the best parameters and the best score
print("Best parameters: ", knn_search.best_params_)
print("Best score: ", knn_search.best_score_)

#Building a neural Network
from keras.models import Sequential
from keras.layers import Dense

# Build the Neural Network model
model = Sequential()
model.add(Dense(64, activation='relu', input_dim=X_train.shape[1]))
model.add(Dense(32, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
#loss is the difference between the actual output vs the predicted output
#optimizer helps you minimize the difference
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, epochs=100, batch_size=10, verbose=0)

# Evaluate the model
train_score = model.evaluate(X_train, y_train, verbose=0)
test_score = model.evaluate(X_test, y_test, verbose=0)
print("Train Accuracy: %.2f%%" % (train_score[1] * 100))
print("Test Accuracy: %.2f%%" % (test_score[1] * 100))

#epochs->iterations
#batch size->number of data points trained at a single time
history = model.fit(X_train, y_train, epochs=200, batch_size=5, verbose=0, validation_data=(X_test, y_test))
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()